{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDbqfd4zny7M"
      },
      "source": [
        "#3. Feature Engineering\n",
        "\n",
        "**i. One HOT Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PIJ4mdIIpDy7"
      },
      "outputs": [],
      "source": [
        "text_feat1 = \"the movie was enjoyable\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnkl6ZBUqE51",
        "outputId": "f7d1c2e4-25e3-42b8-c76b-4a1423a13d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'enjoyable': 0, 'movie': 1, 'the': 2, 'was': 3}\n",
            "\n",
            "One-Hot Encoded Vectors:\n",
            "the: [0, 0, 1, 0]\n",
            "movie: [0, 1, 0, 0]\n",
            "was: [0, 0, 0, 1]\n",
            "enjoyable: [1, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize\n",
        "tokens = text_feat1.split()\n",
        "\n",
        "# Build Vocabulary: Unique words in the corpus\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# One-hot encode\n",
        "one_hot_vectors = []\n",
        "for word in tokens:\n",
        "    vector = [0] * len(vocab)\n",
        "    vector[word_to_index[word]] = 1\n",
        "    one_hot_vectors.append(vector)\n",
        "\n",
        "# Display\n",
        "print(\"Vocabulary:\", word_to_index)\n",
        "print(\"\\nOne-Hot Encoded Vectors:\")\n",
        "for word, vec in zip(tokens, one_hot_vectors):\n",
        "    print(f\"{word}: {vec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64NLhlPcqh_I"
      },
      "outputs": [],
      "source": [
        "# Limitations of One-Hot Encoding:\n",
        "# - High-dimensional and sparse (for large vocab, vectors become huge)\n",
        "# - No information about word meaning (semantic similarity between 'good' and 'great' = 0)\n",
        "# - Vocabulary-specific: unseen words in test set can't be encoded\n",
        "# - Not ideal for downstream deep learning (use embeddings instead)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PidS-Sv0ql8V"
      },
      "source": [
        "**ii. BOW (BAG OF WORDS)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6uct0AqKdq",
        "outputId": "1d480aef-4b7c-4276-da53-0151cbeef0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               text  output\n",
            "0                  great acting and storyline great       1\n",
            "1  poor dialogue and boring scenes poor boring poor       0\n",
            "2                      fantastic direction and cast       1\n",
            "3                        bad script with no emotion       0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'text': [\n",
        "        'great acting and storyline great',\n",
        "        'poor dialogue and boring scenes poor boring poor',\n",
        "        'fantastic direction and cast',\n",
        "        'bad script with no emotion'\n",
        "    ],\n",
        "    'output': [1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RotR5ViBqz1u"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv=CountVectorizer(ngram_range=(1,1))\n",
        "#cv=CountVectorizer(ngram_range=(2,2))\n",
        "cv=CountVectorizer(ngram_range=(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b0v8W6clq25S"
      },
      "outputs": [],
      "source": [
        "bow= cv.fit_transform(data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYqXTi2hq8at",
        "outputId": "fe729a18-4850-4e52-afdf-f7cdc440eb93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'great': 19, 'acting': 0, 'and': 2, 'storyline': 30, 'great acting': 20, 'acting and': 1, 'and storyline': 5, 'storyline great': 31, 'poor': 23, 'dialogue': 12, 'boring': 8, 'scenes': 26, 'poor dialogue': 25, 'dialogue and': 13, 'and boring': 3, 'boring scenes': 10, 'scenes poor': 27, 'poor boring': 24, 'boring poor': 9, 'fantastic': 17, 'direction': 14, 'cast': 11, 'fantastic direction': 18, 'direction and': 15, 'and cast': 4, 'bad': 6, 'script': 28, 'with': 32, 'no': 21, 'emotion': 16, 'bad script': 7, 'script with': 29, 'with no': 33, 'no emotion': 22}\n"
          ]
        }
      ],
      "source": [
        "#vocabulary\n",
        "print(cv.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdDdymwHq-U8",
        "outputId": "3d6c3393-1dd0-43a4-a2e2-85ff4d4beabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 1 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "print(bow[0].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnF_ClwfrCPC",
        "outputId": "94f5fb87-73da-4121-c2ce-0e0777dd811e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 0 0 0 2 1 1 0 1 1 0 0 0 0 0 0 0 0 0 3 1 1 1 1 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "print(bow[1].toarray()) #try more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLNfL-q_rEtM",
        "outputId": "13a7d21b-1427-4cdf-b58b-5f86e1f3a595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.transform([\"great acting and storyline but how\"]).toarray() #out of vocabulary prblm solved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a951WxRIrW_6"
      },
      "source": [
        "**iii. TFIDF (Term Frequency Inverse-Document Frequency)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUYIjwu4rQvi",
        "outputId": "1d1d7238-ea1b-425e-bb0a-6adc52415eb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.39505606, 0.25215917, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.79011212,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.39505606,\n",
              "        0.        ],\n",
              "       [0.        , 0.16261148, 0.        , 0.50952462, 0.        ,\n",
              "        0.25476231, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.76428692, 0.25476231, 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.34578314, 0.        , 0.        , 0.5417361 ,\n",
              "        0.        , 0.5417361 , 0.        , 0.5417361 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
              "        0.4472136 , 0.        , 0.        , 0.4472136 , 0.        ,\n",
              "        0.4472136 ]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "tfidf.fit_transform(data['text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JPRRfz6rivl",
        "outputId": "78edfa3b-eabe-48f4-e5d4-ba0e21cacc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.91629073 1.22314355 1.91629073 1.91629073 1.91629073 1.91629073\n",
            " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
            " 1.91629073 1.91629073 1.91629073 1.91629073]\n",
            "['acting' 'and' 'bad' 'boring' 'cast' 'dialogue' 'direction' 'emotion'\n",
            " 'fantastic' 'great' 'no' 'poor' 'scenes' 'script' 'storyline' 'with']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(tfidf.idf_)\n",
        "print(tfidf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq4Hs96jrorS"
      },
      "source": [
        "**iv. N-Grams Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XRqqbCXcrlOf"
      },
      "outputs": [],
      "source": [
        "text_ngram1 = \"GeeksForGeeks provides great NLP content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CzxBTpKrsJlt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "corpus = [\"GeeksForGeeks provides great NLP content\"]\n",
        "\n",
        "# Create CountVectorizer with n-grams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1))  # unigrams\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWA0rW9csSwg",
        "outputId": "ccaab55e-61cb-431d-e7bb-e479f97217a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'geeksforgeeks': 1, 'provides': 4, 'great': 2, 'nlp': 3, 'content': 0}\n",
            "\n",
            "N-gram Features:\n",
            "[[1 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# Show results\n",
        "print(\"Vocabulary:\\n\", vectorizer.vocabulary_)\n",
        "print(\"\\nN-gram Features:\")\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nDHNxM64sbtf"
      },
      "outputs": [],
      "source": [
        "# Create CountVectorizer with n-grams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))  # unigrams, bigrams\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hr3YrWZssDt",
        "outputId": "429306c4-cbe5-40de-a569-ac4367505570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'geeksforgeeks': 1, 'provides': 7, 'great': 3, 'nlp': 5, 'content': 0, 'geeksforgeeks provides': 2, 'provides great': 8, 'great nlp': 4, 'nlp content': 6}\n",
            "\n",
            "N-gram Features:\n",
            "[[1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# Show results\n",
        "print(\"Vocabulary:\\n\", vectorizer.vocabulary_)\n",
        "print(\"\\nN-gram Features:\")\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YoOQbpL8stmx"
      },
      "outputs": [],
      "source": [
        "# Create CountVectorizer with n-grams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))  # bigrams, now you can try more\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNrhjY7Psy5J",
        "outputId": "3ffb39bc-f508-4528-b108-b9db1b5b70fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'geeksforgeeks provides': 0, 'provides great': 3, 'great nlp': 1, 'nlp content': 2}\n",
            "\n",
            "N-gram Features:\n",
            "[[1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# Show results\n",
        "print(\"Vocabulary:\\n\", vectorizer.vocabulary_)\n",
        "print(\"\\nN-gram Features:\")\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hzP-rYos5mb"
      },
      "source": [
        "**v. Word Embeddings**\n",
        "\n",
        "1. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s4k9oWSFs0Wv",
        "outputId": "b7bedaac-3fdd-4ef8-9ffa-b60ce1720889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: scipy 1.16.0\n",
            "Uninstalling scipy-1.16.0:\n",
            "  Successfully uninstalled scipy-1.16.0\n",
            "Collecting gensim==4.3.2\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n",
            "Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [gensim]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.2 numpy-1.24.3 scipy-1.10.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b6582201e8ed4eac91a5973b11806616",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip uninstall -y gensim numpy scipy\n",
        "!pip install gensim==4.3.2 numpy==1.24.3 scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8N5_01zstNel"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "us0xiErbtVct"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NL44GtotbR9",
        "outputId": "46ef9ac8-66d7-4428-c6b4-cda3daa58231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api #word2vec is trained on google news corpus of 300 dimension and about 3 million words\n",
        "wv=api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I32cLivRtm6F",
        "outputId": "8cde5f1f-605d-4bd4-eb45-13b41b81fd14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cricketing', 0.8372225761413574),\n",
              " ('cricketers', 0.8165745735168457),\n",
              " ('Test_cricket', 0.8094819188117981),\n",
              " ('Twenty##_cricket', 0.8068488240242004),\n",
              " ('Twenty##', 0.7624265551567078),\n",
              " ('Cricket', 0.75413978099823),\n",
              " ('cricketer', 0.7372578382492065),\n",
              " ('twenty##', 0.7316356897354126),\n",
              " ('T##_cricket', 0.7304614186286926),\n",
              " ('West_Indies_cricket', 0.6987985968589783)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar('cricket')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0BLnqNpvARE",
        "outputId": "765a956e-4498-4530-dc76-838626c60d3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.53541523"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.similarity(\"hockey\", \"sports\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V_ib2yPDvC6K"
      },
      "outputs": [],
      "source": [
        "vec=wv[\"king\"] - wv['man'] + wv['woman'] #this results eual to the vectors of Queen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8GgYosOvEwB",
        "outputId": "e786c877-22c2-440a-93ba-8ff82c681046"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51FBz9nFvGsI",
        "outputId": "fc0b48a3-f7e4-42d2-95a8-9edcdeb273ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.645466148853302),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676352500916),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376775860786438),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar([vec])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEFpmlLUvjkP"
      },
      "source": [
        "**ii. GloVe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UTWZ7DZvIlS",
        "outputId": "efe6c197-9e9b-415c-cea4-196d56aef383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading GloVe...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, zipfile, io\n",
        "import os\n",
        "\n",
        "# Download GloVe vectors (100-dimensional)\n",
        "if not os.path.exists(\"glove.6B.100d.txt\"):\n",
        "    print(\"Downloading GloVe...\")\n",
        "    r = requests.get(\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extract(\"glove.6B.100d.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrPd3oMPwsZF",
        "outputId": "7da99c18-a686-4503-b49d-5bb3bfac50a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe vectors...\n",
            "Loaded GloVe.\n"
          ]
        }
      ],
      "source": [
        "# Load GloVe into dictionary\n",
        "print(\"Loading GloVe vectors...\")\n",
        "glove_embeddings = {}\n",
        "with open(\"glove.6B.100d.txt\", 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = vector\n",
        "print(\"Loaded GloVe.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBRR8Mxw366",
        "outputId": "7b43aaf5-5979-4aa9-f771-708cfe24172c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence: GeeksForGeeks provides great nlp content\n",
            "GloVe Embedding Vector (100-d):\n",
            "[-2.55629003e-01  5.10675013e-01  1.14457503e-01 -7.91724995e-02\n",
            "  3.21207523e-01 -2.47885004e-01  5.01724966e-02 -2.46722504e-01\n",
            "  1.56765580e-02 -3.46251756e-01  4.79699969e-02 -4.49602485e-01\n",
            " -2.03704998e-01 -2.04105005e-01  2.68669993e-01 -1.09435499e-01\n",
            "  2.83295006e-01 -1.01142496e-01 -5.21988515e-03  5.25447488e-01\n",
            " -4.81329739e-01 -4.92179990e-02  1.30396634e-01 -1.61736012e-01\n",
            " -1.58252507e-01 -6.54775053e-02  1.36207491e-02  3.54979992e-01\n",
            " -4.22390014e-01 -2.93450058e-02 -3.79514992e-01  5.23430705e-01\n",
            " -1.35989994e-01 -2.31747493e-01  2.57324994e-01  1.99144363e-01\n",
            " -1.03391252e-01  2.80031502e-01 -3.81422520e-01 -1.33595005e-01\n",
            " -8.55274871e-03 -1.48388416e-01 -4.95285094e-02 -4.30911988e-01\n",
            " -1.45562500e-01 -2.05320001e-01 -1.25419796e-02 -3.22815001e-01\n",
            "  2.35306740e-01 -2.05952004e-01  2.91929990e-02 -2.00350061e-02\n",
            " -1.86074972e-02  4.11282480e-01  5.82964858e-03 -1.18292499e+00\n",
            "  8.92475024e-02 -5.08810043e-01  1.24527752e+00 -1.41422506e-02\n",
            "  2.62813747e-01  1.02244981e-01 -2.51358986e-01 -2.88865030e-01\n",
            "  7.83902526e-01  8.90251249e-04  3.27852517e-01 -2.32887506e-01\n",
            "  3.08041245e-01  1.56985074e-02  7.94514939e-02  1.48817509e-01\n",
            "  5.36838531e-01  9.17872488e-02  2.91408241e-01 -2.50762999e-01\n",
            "  1.37487445e-02 -1.80016756e-01 -4.65052485e-01 -1.11274995e-01\n",
            "  4.61525097e-02 -3.19334865e-01 -6.26350045e-02 -6.82522580e-02\n",
            " -9.90617514e-01  1.85096502e-01  2.99911737e-01 -1.45765737e-01\n",
            "  1.24225114e-03  1.06385760e-01  1.31575018e-02 -2.42974963e-02\n",
            " -7.22342581e-02  7.05875009e-02 -2.81888485e-01 -4.28875029e-01\n",
            " -3.90189976e-01 -8.44885051e-01  5.59484959e-01  6.15147471e-01]\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"GeeksForGeeks provides great nlp content\"\n",
        "\n",
        "# Average GloVe embedding\n",
        "def sentence_embedding(sentence, embedding_dict, dim=100):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [embedding_dict[word] for word in words if word in embedding_dict]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "embedding_vector = sentence_embedding(text, glove_embeddings)\n",
        "print(\"\\nSentence:\", text)\n",
        "print(\"GloVe Embedding Vector (100-d):\")\n",
        "print(embedding_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_uU7eVjwfgA"
      },
      "source": [
        "**6. Dependency Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N6BHRoQXv4v0"
      },
      "outputs": [],
      "source": [
        "text_adv2 = \"The quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xZ5hlX2MxDvV",
        "outputId": "81fb01ca-bfd4-4e14-9f08-b85bf8c83ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Found existing installation: numpy 2.3.2\n",
            "Uninstalling numpy-2.3.2:\n",
            "  Successfully uninstalled numpy-2.3.2\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Installing collected packages: numpy, scipy\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [scipy]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3 scipy-1.10.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "48566065a7d14c14b028b9532f58bc0a",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Install spaCy and download model\n",
        "!pip install spacy\n",
        "!pip uninstall -y numpy scipy\n",
        "!pip install numpy==1.24.3 scipy==1.10.1\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAas8UNJxDMz",
        "outputId": "3a1c0b34-5a42-42ae-a53b-e39cd2560189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependency Parsing Result:\n",
            "\n",
            "Token        Dep        Head         POS     \n",
            "---------------------------------------------\n",
            "The          det        fox          DET     \n",
            "quick        amod       fox          ADJ     \n",
            "brown        amod       fox          ADJ     \n",
            "fox          nsubj      jumps        NOUN    \n",
            "jumps        ROOT       jumps        VERB    \n",
            "over         prep       jumps        ADP     \n",
            "the          det        dog          DET     \n",
            "lazy         amod       dog          ADJ     \n",
            "dog          pobj       over         NOUN    \n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function for dependency parsing\n",
        "def parse_sentence(text):\n",
        "    doc = nlp(text)\n",
        "    print(f\"{'Token':<12} {'Dep':<10} {'Head':<12} {'POS':<8}\")\n",
        "    print(\"-\" * 45)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {token.dep_:<10} {token.head.text:<12} {token.pos_:<8}\")\n",
        "\n",
        "# Example usage\n",
        "text_adv2 = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(\"Dependency Parsing Result:\\n\")\n",
        "parse_sentence(text_adv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoitOQeVxMau",
        "outputId": "16a0884b-1547-4081-9257-0845d6c1d63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependency Parsing Result:\n",
            "\n",
            "Token        Dep        Head         POS     \n",
            "---------------------------------------------\n",
            "The          det        fox          DET     \n",
            "quick        amod       fox          ADJ     \n",
            "brown        amod       fox          ADJ     \n",
            "fox          nsubj      jumps        NOUN    \n",
            "jumps        ROOT       jumps        VERB    \n",
            "over         prep       jumps        ADP     \n",
            "the          det        dog          DET     \n",
            "lazy         amod       dog          ADJ     \n",
            "dog          pobj       over         NOUN    \n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "text_adv2 = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(\"Dependency Parsing Result:\\n\")\n",
        "parse_sentence(text_adv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg7BX0hJxlwA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
